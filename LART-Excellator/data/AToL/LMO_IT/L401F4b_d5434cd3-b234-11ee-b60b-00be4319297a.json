{
    "meta": {
        "version_id": "LmoIta_Ita_IT",
        "version_no": "0.3.4",
        "app_version": "0.3.4",
        "researcher_id": "Lissander",
        "research_location": "Fiorine",
        "participant_id": "L401F4b",
        "consent": true,
        "date": "2024-01-13"
    },
    "presentation order_atolRatingsMaj": [
        "beauty",
        "sistem",
        "angularity",
        "structure",
        "flow",
        "logic",
        "ambiguity",
        "elegance",
        "grace",
        "appeal",
        "smoothness",
        "fluency",
        "precision",
        "harshness",
        "pleasure"
    ],
    "Ratings_atolRatingsMaj": {
        "rating_Italiano_ambiguity": "32.2368421052632",
        "rating_Italiano_angularity": "32.0394736842105",
        "rating_Italiano_appeal": "11.6447368421053",
        "rating_Italiano_beauty": "0.263157894736842",
        "rating_Italiano_elegance": "65.1315789473684",
        "rating_Italiano_flow": "12.1052631578947",
        "rating_Italiano_fluency": "76.5789473684211",
        "rating_Italiano_grace": "77.3026315789474",
        "rating_Italiano_harshness": "75.1973684210526",
        "rating_Italiano_logic": "22.5657894736842",
        "rating_Italiano_pleasure": "11.1184210526316",
        "rating_Italiano_precision": "48.8157894736842",
        "rating_Italiano_sistem": "22.3026315789474",
        "rating_Italiano_smoothness": "21.1842105263158",
        "rating_Italiano_structure": "82.1710526315789"
    },
    "presentation order_atolRatingsRml": [
        "precision",
        "elegance",
        "structure",
        "fluency",
        "beauty",
        "grace",
        "sistem",
        "pleasure",
        "smoothness",
        "angularity",
        "harshness",
        "flow",
        "ambiguity",
        "logic",
        "appeal"
    ],
    "Ratings_atolRatingsRml": {
        "rating_rml_ambiguity": "33.4210526315789",
        "rating_rml_angularity": "23.8157894736842",
        "rating_rml_appeal": "44.2763157894737",
        "rating_rml_beauty": "35.4605263157895",
        "rating_rml_elegance": "35.7236842105263",
        "rating_rml_flow": "62.6315789473684",
        "rating_rml_fluency": "63.75",
        "rating_rml_grace": "35.1973684210526",
        "rating_rml_harshness": "33.0263157894737",
        "rating_rml_logic": "38.2894736842105",
        "rating_rml_pleasure": "42.9605263157895",
        "rating_rml_precision": "55.3289473684211",
        "rating_rml_sistem": "57.7631578947368",
        "rating_rml_smoothness": "45.3289473684211",
        "rating_rml_structure": "36.1842105263158"
    }
}